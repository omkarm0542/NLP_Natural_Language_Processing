# -*- coding: utf-8 -*-
"""M12 Text preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SdyWe94cGVP_oJaSVYd3KP8A8Eaifr4-
"""

pip install nltk

import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')

print(stopwords.words("english"))

len(stopwords.words('english'))

s = "hai how are you".split()

s

"**".join(s)

"""###removing stop words from a sentence"""

nltk.download('punkt')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
  
example_sent = """We are learning Natural Language Processing as part of
                Data Science course track."""
  
stop_words = set(stopwords.words('english'))
  
word_tokens = word_tokenize(example_sent)

word_tokens

#filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]
  
filtered_sentence = []
  
for w in word_tokens:
    if w not in stop_words:
        filtered_sentence.append(w)
  
print(word_tokens)
print(filtered_sentence)

" ".join(filtered_sentence)

filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]

filtered_sentence

p = "!@#$%^&*()?'"
#remove punctuations from the given sentence



"""### Tokenization using Pythons split() function"""

# Word Tokenization
text = '''Once there lived a greedy man in a small town. (He was very rich, and he loved gold and all things fancy. But he loved his daughter more than anything. One day, he chanced upon a fairy. The fairy’s hair was caught in a few tree branches. He helped her out, but as his greediness took over, he realised that he had an opportunity to become richer by asking for a wish in return (by helping her out). The fairy granted him a wish. He said, “All that I touch should turn to gold.” And his wish was granted by the grateful fairy.'''

tokens = text.split()

print(tokens)
print("No.of tokens : ", len(tokens))

# Sentence Tokenization
text = '''Once there lived a greedy man in a small town?  He was very rich, and he loved gold and all things fancy. But he loved his daughter more than anything. One day, he chanced upon a fairy. The fairy’s hair was caught in a few tree branches. He helped her out, but as his greediness took over, he realised that he had an opportunity to become richer by asking for a wish in return (by helping her out). The fairy granted him a wish. He said, “All that I touch should turn to gold.” And his wish was granted by the grateful fairy.'''
sentences = text.split('.')

print(sentences)
print("No.of sentences : ", len(sentences))

"""### 2. Tokenization using Regular Expressions (RegEx)"""

import re

#word tokenization
tokens = re.findall("[\w']+", text)  
print(tokens)
print("No.of tokens : ", len(tokens))

#sentence tokenization
sentences = re.compile('[.?!] ').split(text)
print(sentences)
print("No.of sentences : ", len(sentences))

"""### 3. Tokenization using NLTK"""

!pip install --user -U nltk

import nltk
nltk.download('punkt')

#word tokenization
from nltk.tokenize import word_tokenize 
tokens = word_tokenize(text)
print(tokens)
print("No.of tokens : ", len(tokens))

V = set(tokens)

len(V)

len(V)

#sentence tokenization
from nltk.tokenize import sent_tokenize
sentences = sent_tokenize(text)
print(sentences)
print("No.of sentences : ", len(sentences))

#Stemming words
from nltk.stem import PorterStemmer

#create an object of class PorterStemmer
porter = PorterStemmer()
print(porter.stem("saw"))
print(porter.stem("troubles"))

# code to stem the sentence

sent = "Stemmer algorithm is not considered as best as it gives meaningless tokens sometimes"
stemmed = []
tokens = word_tokenize(sent)
for token in tokens:
  stemmed.append(porter.stem(token))

print(" ".join(stemmed))

"""create a function which takes a sentence and returns the stemmed sentence.


> Indented block


---


"""

sentence="Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success."

tokens = word_tokenize(sentence)

from nltk.stem import PorterStemmer
porter  = PorterStemmer()
print(sentence)

stemtokens = [porter.stem(t) for t in tokens]
stemtokens

from nltk.stem import PorterStemmer
porter = PorterStemmer()

sentence="Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success."

from nltk.tokenize import word_tokenize

def stemSentence(sentence):
    token_words=word_tokenize(sentence)
    print(token_words)

    stem_sentence=[]
    for word in token_words:
        stem_sentence.append(porter.stem(word))
        stem_sentence.append(" ")
    return "".join(stem_sentence)

x=stemSentence(sentence)
print("Sentence after stemming :\n", x)

#Extend the above program to do stemming of the given document.

nltk.download('all')

from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

print(wordnet_lemmatizer.lemmatize("cats"))
print(wordnet_lemmatizer.lemmatize("troubles"))

import nltk
from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

sentence = "He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun."
punctuations="?:!.,;"
token_words = nltk.word_tokenize(sentence)
print(token_words)

lemma_sentence=[]
for word in token_words:
  lemma_sentence.append(wordnet_lemmatizer.lemmatize(word))
  lemma_sentence.append(" ")

print("lemmas of tokens: ", ''.join(lemma_sentence))

wordnet_lemmatizer.lemmatize('better')

"""#add why lemmatizer is not working properly"""

import nltk
nltk.download('all')

from nltk.corpus import stopwords

stopwords = stopwords.words('english')

sentence = "We are learning Text processing as part of data science training in module 12."

tokens = word_tokenize(sentence.lower())

' '.join([t for t in tokens if t not in stopwords])

s = ['I', 'am', 'learning', 'NLP']

''.join(s)

